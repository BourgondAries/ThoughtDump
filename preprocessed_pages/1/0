Math. Notation.
<br><br>
<h1>New Home</h1>
Mathematics is plagued by inconsistencies that obscure mathematical meaning. One such example is the way we write matrix multiplication. The standard notation is write a juxtaposition of matrices: $AB$, but this makes it hard to distinguish multi-letter symbols. Thus, some authors choose $A\cdot B$, but this is inconsistent with the dot-product for vectors, in which the first argument is transposed: $A\cdot B = T(A) \space B$. Another inconsistency is the use of the exponentiation operator. Where a transpose is often written $A^T$. The exponential is supposed to mean repetition of the same expression. The laplacian operator breaks this: $\nabla^2 = \nabla\cdot\nabla$ while also using the vector dot-product inconsistency. Inconsistencies within this realm are common.
In addition to solving inconsistencies, we must allow the use of multi-letter symbols to make mathematics easier to read.
<br>
I propose a simple operator based and tensor friendly mathematics that solves these problems.

<br><br>
<h1>Symbols</h1>
A symbol is a string of characters in any language. They are always grouped close together. They can consist of any characters except digits and delimiters.
$$ xyz $$
is a single symbol. $x$ can also be a symbol on its own, but has to stand out. There must be a clear separation like in the expression $x\space y$ or $x\cdot y$.

<br><br>
<h1>Operators</h1>
Operators are symbols that are defined as operators. An operator operates on its left or right side, or both.
<br>
Postfix operator: $!$ as in $5!$ or $(n)!$.<br>
Prefix operator: $\partial_1$ as in $\partial_1 f$.<br>
Binary operators: $+$ as in $1 + 2$.<br>

<br><br>
<h1>Tensors and Tuples</h1>
Tensors are written using square brackets.<br>
Rank 1 tensor:
$$ [x_1, x_2, x_3] $$
Rank 2 tensor:
$$ \begin{bmatrix}
x_{11}       & x_{12} \\
x_{21}       & x_{22}
\end{bmatrix}
$$
Rank 3 tensor:
$$
\begin{bmatrix}
[x_{111}, x_{112}] & [x_{121}, x_{122}] \\
[x_{211}, x_{212}] & [x_{221}, x_{222}]
\end{bmatrix}
$$
And so on.<br>
Tuples denote column vectors: $(x_1, x_2, ..., x_n) = T[x_1, x_2, ..., x_n]$. Tuples are written using parentheses. A single-element tuple is written $(x,)$. The T operator denotes transposition. Vectors and tuples may have trailing commas.

<br><br>
<h1>Tensor Extractor</h1>
The tensor extractor is an operator that extracts an entry from a tensor.
$$ $ $$
is the tensor extraction operator.
$$ $_3 $$
Denotes the third element of the tensor:
$$ $_3 (x,y,z) = z $$
where $(x,y,z)$ is a tuple. A tuple must by necessity be a column vector.
Commas in the tensor extractor denote dimension:
$$ $_{2,3}
\begin{bmatrix}
a, & b, & c \\
d, & e, & f \\
g, & h, & i
\end{bmatrix}
=
f
$$
Tensor extractors can also operate on ranges, given by colons:
$$ $_{2,2:3}
\begin{bmatrix}
a, & b, & c \\
d, & e, & f \\
g, & h, & i
\end{bmatrix}
=
[e, f]
$$
Omitted trailing dimensions means "all entries":
$$ $_{2}
\begin{bmatrix}
a, & b, & c \\
d, & e, & f \\
g, & h, & i
\end{bmatrix}
=
[d, e, f]
$$
To access all elements in previous dimensions:
$$ $_{:,2}
\begin{bmatrix}
a, & b, & c \\
d, & e, & f \\
g, & h, & i
\end{bmatrix}
=
\begin{bmatrix}
b \\
e \\
h
\end{bmatrix}
$$
Alternatively one can use implicit end values via omittance:
$$ $_{2:,2}
\begin{bmatrix}
a, & b, & c \\
d, & e, & f \\
g, & h, & i
\end{bmatrix}
=
\begin{bmatrix}
e \\
h
\end{bmatrix}
$$
This is particularly useful when the size is not known beforehand. All indices start at 1. Negative indices can be used, where $-1$ is the last element in that dimension.
You can also specify multiple ranges using a semicolon:
$$ $_{2:,1;3:}
\begin{bmatrix}
a, & b, & c \\
d, & e, & f \\
g, & h, & i
\end{bmatrix}
=
\begin{bmatrix}
d, & f \\
g, & i
\end{bmatrix}
$$
Extracting the entire tuple is a matter of specifying an all-dimensional element:
$$ $_{:}
\begin{bmatrix}
a, & b, & c \\
d, & e, & f \\
g, & h, & i
\end{bmatrix}
=
\begin{bmatrix}
a, & b, & c \\
d, & e, & f \\
g, & h, & i
\end{bmatrix}
$$

<br><br>
<h1>Functions</h1>
Functions are expressions that map tensors to tensors using tensor extractors. A function always refers to its parameters via those extractors. A function is always a right-facing operator.
<br>
Example 1:
$$ F = $_1 $$
This function maps the first argument of its tuple. If we supply a tuple, we get $ F(x) = x $.

<br>
Example 2:
$$ F = $_2 $$
This function maps the second argument of its tuple. If we supply a tuple, we get $ F(x, y) = y $.

<br>
Example 3:
$$ F = $_2 - $_1 $$
If we supply a tuple, we get $ F(x, y) = y - x $.

<br>
Example 4:
$$ F = $($_2 $_1) $$
If we supply a tuple, we get $ F(x, y) = y\cdot x $. Notice how we provide an outer $\$$. The reason for doing this is to apply insertion of A into all extractors:
$$ $($_2$_1)A \neq $_2$_1A $$
as
$$ $($_2$_1)A = A_2A_1 \neq $_2$_1A = $_2A_1 = A_{1,2} $$

<br>
Example 5:
$$ F = $(@_2 A\space $_1) $$
If we supply a tuple, we get $ F(x) = A_2 x $. Notice how we provide a @. This symbol denotes eager evaluation and won't be filled in by the outer $\$$.

<br><br>
<h1>Implicit Pattern</h1>
The pattern
$$ $(X\space $_:) $$
can always be written as
$$ X $$
since there is no semantic difference.

<br><br>
<h1>Sums and Products</h1>
Sums and products are operators:

$$ \sum_{i=n}^N = $($_:(i) + $_:(i+1) + ... + $_:(N)) $$

$$ \prod_{i=n}^N = $($_:(i) \cdot $_:(i+1) \cdot ... \cdot $_:(N)) $$

<br><br>
<h1>Partial Derivatives</h1>
Partial derivatives are operators that operate on a function with respect to a parameter index:

$$ \partial_1 $($_1 $_2) = $($_2) $$

<br><br>
<h1>Total Derivatives</h1>
The total derivative operator is $d$. It is defined as:
$$ d = $\left(\sum_{i=0}^\infty \partial_i $_: d(i)\right) $$

<br><br>
<h1>Nabla</h1>
The nabla operator $\nabla$ is defined as:
$$ \nabla = (\partial_1, \partial_2, ...) $$
It is a column vector of all partial derivative operators.<br>
The operator can be limited by an extractor:
$$ @_{:3}\nabla = (\partial_1, \partial_2, \partial_3) $$
For convenience we define $\bar{\nabla}$ to avoid cumbersome expressions:
$$ \bar{\nabla} = $(@_{:size($_:)}\nabla \space $_:) $$

<br><br>
<h1>Gradient</h1>
The gradient is defined by:
$$ grad = \bar{\nabla} $$

<br><br>
<h1>Divergence</h1>
The divergence is defined by:
$$ div = T \space \nabla $$

<br><br>
<h1>Curl</h1>
The curl is defined by:
$$ curl = @_{1:3}\nabla \times $$
where $\times$ is the cross product.

<br><br>
<h1>Cross Product</h1>
The cross product is defined only for a 3-vector:
$$ $($_1 \times $_2) = $(\det(T(@_{1:3}\nabla), T$_1, T$_2)) $$

<br><br>
<h1>Determinant</h1>
The determinant is defined by the $det$ operator:
$$ \det = $\left(\sum_{i=1}^{cols($_:)}{(-1)^{i-1}@_{1,i}$_:\det(@_{2:,1:i-1;i+1:}$_:)}\right) $$

<br><br>
<h1>Exponentiation</h1>
Exponents always mean repetition on symbols, otherwise exponentiation:
$$ \sin^3 = \sin\sin\sin $$

<br><br>
<h1>Jacobian</h1>
The Jacobian matrix is defined in terms of nabla:
$$ J = \bar{\nabla}\space T $$

<br><br>
<h1>Hessian</h1>
The Hessian matrix is defined in terms of nabla:
$$ H = \bar{\nabla}(T \space\bar{\nabla}) $$

<br><br>
<h1>Laplacian</h1>
The laplacian operator is defined as:
$$ L = (T\space \nabla) \nabla $$

<br><br>
<h1>Integrals</h1>
Integrals are written in the same form as in old mathematics:
$$ \int_{start}^{stop} d(v) $$

<br><br>
<h1>Vector-Vector Implicitation</h1>
An implicitation operation is performed when two elements are juxtaposed. This means separation by parenthesis or space: $(x) y$ or $x \space y$.<br>
Here is the definition of vector-vector juxtaposition:
$$ (x_1, x_2, ..., x_n)(y_1, y_2, ..., y_n) = (x_1, x_2, ..., x_n) A = (x_1A, x_2A,...x_nA) $$
The reason for this is to have consistency between vector-valued functions and their tuple-arguments:
$$ f = (f_1, f_2,..., f_n) $$
$$ \Rightarrow f(x,y,z) = (f_1, f_2,..., f_n)(x,y,z) = (f_1(x,y,z), f_2(x,y,z),..., f_n(x,y,z)) $$

<br><br>
<h1>Diagonalization</h1>
Diagonalization of a vector:
$$ diag $$
Operates on a column vector (tuple) and produces a symmetric matrix using the values of the vector as diagonals. If a matrix is given, the vector is extracted from the diagonals.
