Math. Notation.
<br><br>
<h1>New Home</h1>
Mathematics is plagued by inconsistencies that obscure mathematical meaning. One such example is the way we write matrix multiplication. The standard notation is write a juxtaposition of matrices: $AB$, but this makes it hard to distinguish multi-letter symbols. Thus, some authors choose $A\cdot B$, but this is inconsistent with the dot-product for vectors, in which the first argument is transposed: $A\cdot B = T(A) \space B$. Another inconsistency is the use of the exponentiation operator. Where a transpose is often written $A^T$. The exponential is supposed to mean repetition of the same expression. The laplacian operator breaks this: $\nabla^2 = \nabla\cdot\nabla$ while also using the vector dot-product inconsistency. Another inconsistency is when one integrates over the differential k-form where argument insertion is implied. This implicitness in some of mathematics makes it difficult. Inconsistencies within this realm are common.
In addition to solving inconsistencies, we must allow the use of multi-letter symbols to make mathematics easier to read.
<br>
I propose a simple lambda-calculus based mathematics that solves these problems.

<br><br>
<h1>Symbols</h1>
A symbol is a string of characters in any language. They are always grouped close together. They can consist of any characters except digits and delimiters.
$$ xyz $$
is a single symbol. $x$ can also be a symbol on its own, but has to stand out. There must be a clear separation like in the expression $x\space y$ or $x\cdot y$.

<br><br>
<h1>Operators</h1>
Operators are symbols that are defined as operators. An operator operates on its left or right side, or both.
<br>
Postfix operator: $ ! $ as in $ 5! $ or $ (n)! $.<br>
Prefix operator: $ \partial_1 $ as in $ \partial_1 f $.<br>
Binary operators: $ + $ as in $ 1 + 2 $.<br>

<br><br>
<h1>Tensors and Tuples</h1>
Tensors are written using square brackets.<br>
Rank 1 tensor:
$$ [x_1, x_2, x_3] $$
Rank 2 tensor:
$$ \begin{bmatrix}
x_{11}       & x_{12} \\
x_{21}       & x_{22}
\end{bmatrix}
$$
Rank 3 tensor:
$$
\begin{bmatrix}
[x_{111}, x_{112}] & [x_{121}, x_{122}] \\
[x_{211}, x_{212}] & [x_{221}, x_{222}]
\end{bmatrix}
$$
And so on.<br>
Tuples are written using parentheses: $ (x_1, x_2, ..., x_n) $. A single-element tuple is written $ (x) $ or just $ x $.

<br><br>
<h1>Transpose</h1>
The $ T $ operator denotes the transpose of a tensor.

<br><br>
<h1>Functions</h1>
Functions are expressions that map mathematical objects. A function always refers to its parameters via lambda enumerators. A function is always a right-facing operator.
<br>
Example 1:
$$ F = \lambda_1 = \lambda $$
This function maps the first argument to itself. If we supply a tuple, we get $ F(x) = x $.
No subscript on the lambda means $ \lambda_1 $.

<br>
Example 2:
$$ F = \lambda_2 $$
This function maps the second argument of its tuple. If we supply two arguments we get $ F(x)(y) = F(x, y) = y $.

<br>
Example 3:
$$ F = \lambda_2 - \lambda_1 $$
If we supply one argument, we get $ F(x) = \lambda_1 - x $. Every time an argument is provided, $ \lambda_1 $ is replaced by the argument. All other $ \lambda_n $ are decremented.
If we supply two arguments, we get $ F(x)(y) = F(x, y) = y - x $.

<br>
Example 4:
$$ F = \lambda_2 \lambda_1 $$
If we supply a tuple, we get $ F(x)(y) F(x, y) = y\cdot x $.

<br>
Example 5:
$$ F = A_2 \space \lambda_1 $$
If we supply an argument, we get $ F(x) = A_2 \space x $.

<br><br>
<h1>Lambda Signifier</h1>
The lambda signifier is given by $ (: $. It is used in expressions where one must denote that this is a function. For single variables we can drop it because it is implicit:
$$ f = \lambda_1 + \lambda_2 = (: \lambda_1 + \lambda_2) $$
Because when using $ f $ in an expression, $ (: $ is implied:
$$ f (9, 3) = (: f)(9,3) $$
$ (: $ becomes $ ( $ when the expression becomes $ \eta $-reducable.

<br><br>
<h1>$ \eta $-reduction</h1>
The pattern
$$ (: X \space \lambda) $$
can always be written as
$$ X $$
since there is no semantic difference.<br>
This is called an $ \eta $-reduction. We may write:
$$ (: X \space \lambda) \rightarrow_\beta X $$
If and only if $ X $ is a function or can be regarded as such.

<br><br>
<h1>Scope</h1>
A lambda variable is bound to the closest declaration of $ (: $.
$$ (: \lambda \cdot (: 5 + \lambda) \lambda) $$
The first lambda is bound to the outer function. The second lambda bound to the inner function, and the last lambda is bound to the outer function. In this case, the third lambda is passed to the inner function.
$$ (: \lambda \cdot (: 5 + \lambda) \lambda) 3 \rightarrow_\beta (: 3 \cdot (: 5 + \lambda) 3) \rightarrow_\eta (3 \cdot (: 5 + \lambda) 3) \rightarrow_{\beta\eta} (3 \cdot (5 + 3)) $$
We can state the number of lambda variables as well, this is useful if we want to consume a variable and not use it.
$$
(\lambda_{10}: \lambda_9 + \lambda_4)(1,2,3,4,5,6,7,8,9) \\
\rightarrow_\beta (\lambda_9: \lambda_8 + \lambda_3)(2,3,4,5,6,7,8,9) \\
\rightarrow_\beta (\lambda_8: \lambda_7 + \lambda_2)(3,4,5,6,7,8,9) \\
\rightarrow_\beta (\lambda_7: \lambda_6 + \lambda_1)(4,5,6,7,8,9) \\
\rightarrow_\beta (\lambda_6: \lambda_5 + 4)(5,6,7,8,9) \\
\rightarrow_\beta (\lambda_5: \lambda_4 + 4)(6,7,8,9) \\
\rightarrow_\beta (\lambda_4: \lambda_3 + 4)(7,8,9) \\
\rightarrow_\beta (\lambda_3: \lambda_2 + 4)(8,9) \\
\rightarrow_\beta (\lambda_2: \lambda_1 + 4)(9) \\
\rightarrow_\beta (\lambda_1: 9 + 4)(9) \\
= (\lambda: 9 + 1)
$$
A function can also accept arbitrary lambda expressions:
$$ (\lambda : \lambda) (\lambda_2) = (\lambda_2 : \lambda_2) $$

<br><br>
<h1>Tensors II</h1>
Tensors are to be considered functions of tuples. Let's start with vectors:
$$ \left(: \begin{bmatrix} \lambda_1 \\ \lambda_2 \\ \vdots \\ \lambda_n \end{bmatrix}\right) $$
The same can be done for matrices and higher order tensors. The use of this becomes clear when we define the derivative of a function that takes in a vector:
$$ f = A \cdot \lambda $$
where A is an arbitrary matrix, and $ \lambda $ is a column vector. Assume the length of the vector is 3.
$$ \mathrm{d} \space f = \partial_1 f \space \lambda \space \mathrm{d} \space \lambda $$
Let $ V $ be the vector function that transforms a tuple to a vector.
$$ \mathrm{d} \space f \space V = \partial_1 f \space V \space \mathrm{d} \space V $$
$$ \mathrm{d} \space f \space V = (1: A) \space V \space \mathrm{d} \space V $$
$$ \mathrm{d} \space f \space V = A \space \mathrm{d} \space V $$
$$ \mathrm{d} \space f \space V = A \space (\partial_1\space V \space \mathrm{d}\space\lambda_1 + \partial_2\space V \space \mathrm{d}\space\lambda_2 + \partial_3\space V \space \mathrm{d}\space\lambda_3) $$

<br><br>
<h1>Sums and Products</h1>
Sums and products are operators:

$$ \sum_{i=n}^N = (: \lambda(i) + \lambda(i+1) + ... + \lambda(N)) $$

$$ \prod_{i=n}^N = (: \lambda(i) \cdot \lambda(i+1) \cdot ... \cdot \lambda(N)) $$

<br><br>
<h1>Partial Derivatives</h1>
Partial derivatives are operators that operate on a function with respect to a parameter index:

$$ \partial_1 (: \lambda_1 \lambda_2) = (: \lambda_2) $$

$$ \partial_1 (: \exp(\lambda_1 \cdot a)) = (: a\space \exp(\lambda_1 \cdot a)) $$

<br><br>
<h1>Total Derivatives</h1>
The total derivative operator is $d$. It is defined as:
$$ \mathrm{d} = \sum_{i=1}^{arity(\lambda_1)} \partial_i \lambda_1 \lambda_{2:arity(\lambda_1)+1} \mathrm{d}(\lambda_{i+1}) $$
If we derive the expression $ f \space g $, where $ g $ is a function of multiple variables:
$$ \mathrm{d} \space (f \space g) = \sum_{i=1}^{3} \partial_i (f \space g) \lambda_{1:3} \mathrm{d}(\lambda_{i}) $$
$$ \mathrm{d} \space (f \space g) (x, y, z) = \sum_{i=1}^{3} \partial_i (f \space g) (x, y, z)  \mathrm{d}((x, y, z)_i) $$
I need a better formulation. I'm missing the recursive part.
In fact, it would be most appropriate to call:
$$ \mathrm{d} \space f (g(h(j(x,y,z)))) $$
Now I think the problem comes when we try to define stuff partially: We can't do
$$ \mathrm{d} \space f (g) $$
because it means $ g $ is our final variable.
The definition only accepts two arguments. How can it be n-ary?
One method is to do the following:
$$ \mathrm{d} \space f (g \space \lambda_{1:arity(g)}) $$
You can postpone defining lambda until a later point.

<br><br>
<h1>Apply</h1>
We can define the apply-function as:
$$ A = (\lambda_1: \lambda_1 \space \lambda_{2:arity(\lambda_1) + 1} ) $$

<br><br>
<h1>Nabla</h1>
The nabla operator $\nabla$ is defined as:
$$ \nabla = T [\partial_1, \partial_2, ..., \partial_{arity(\lambda)}] \lambda $$
It is a column vector of all partial derivative operators.<br>

<br><br>
<h1>Gradient</h1>
The gradient is defined by:
$$ grad = \nabla $$

<br><br>
<h1>Divergence</h1>
The divergence is defined by:
$$ div = T \space \nabla $$

<br><br>
<h1>Curl</h1>
The curl is defined by:
$$ curl = \nabla \times $$
where $\times$ is the cross product.

<br><br>
<h1>Cross Product</h1>
The cross product is defined only for a 3-vector:
$$ C = \lambda_1 \times \lambda_2 = \det \space [\nabla, \lambda_1, \lambda_2] $$
This will work fine since $ \det $ works with indices, in this case, we will get the same indices as we would have with a matrix.

<br><br>
<h1>Determinant</h1>
The determinant is defined by the $det$ operator:
$$ \det = \sum_{i=1}^{cols(\lambda)}{(-1)^{i-1}(\lambda)_{1,i}\det((\lambda)_{2:,:i-1;i+1:})} $$

<br><br>
<h1>Exponentiation</h1>
Exponents always mean repetition on symbols, otherwise exponentiation:
$$ \sin^3 = \sin\sin\sin $$

<br><br>
<h1>Jacobian</h1>
The Jacobian matrix is defined in terms of nabla:
$$ J = \nabla \space T $$

<br><br>
<h1>Hessian</h1>
The Hessian matrix is defined in terms of nabla:
$$ H = \nabla(T \nabla \lambda) $$

<br><br>
<h1>Laplacian</h1>
The laplacian operator is defined as:
$$ L = (T\space \nabla) \nabla $$

<br><br>
<h1>Integrals</h1>
Integrals are written in the same form as in old mathematics:
$$ \int_{start}^{stop} d(v) $$

<br><br>
<h1>Diagonalization</h1>
Diagonalization of a vector:
$$ diag $$
Operates on a column vector and produces a symmetric matrix using the values of the vector as diagonals. If a matrix is given, the vector is extracted from the diagonals.

<br><br>
<h1>Properties</h1>

<br><br>
<h1>Kleene-Rosser Paradox</h1>
The lambda calculus is fundamentally inconsistent as shown by the Kleene-Rosser paradox:
$$ k = \neg(\lambda^2) $$
$$ \Rightarrow k \space k = \neg(k^2) $$
We can interpret this as recursion:
$$ k \space k = p = \neg p = \neg^2 p = ... $$

<br><br>
<h1>Turing Completeness</h1>
Lambda calculus is Turing complete:
$$ \Omega \equiv \lambda_2 \space \lambda_1^2 \space \lambda_2 $$
$$ T \equiv \Omega \space \Omega $$
Where $t$ is a single-step Turing interpreter:
$$ T \space t \space \lambda \rightarrow_\beta t \space t \space ... \space t \space \lambda $$
simulates the turing machine.<br>
<br><br>
<h1>Applications</h1>
We'll explore a few applications of the above defintions.

<br><br>
<h1>Taylor Polynomial</h1>
The general taylor polynomial can now be written as:
$$ Y = \sum_{n=0}^\infty \frac{(T\space \lambda_{2:arity(\lambda_1)+1} \space \nabla \space)^n \lambda_1}{n!}(\lambda_{arity(\lambda_1)+1:2arity(\lambda_1)+1}) $$
This can be used as:
$$ Y(f, a)(x) \approx f(x) $$
or
$$ f(x-a) \sim \sum_{n=0}^\infty \frac{(T\space (x-a)\space \nabla\space)^nf}{n!}(a) $$
$Y$ creates a new operator. The exponent of $\$$ denotes escaped insertion.

<br><br>
<h1>Fourier/Laplace Transform</h1>
The transforms can be written:
$$ F = \int_{-\infty}^{\infty} d(t) e^{-i\space w\space t} $$
$$ L = \int_{0}^{\infty} d(t) e^{-s\space t} $$

<br><br>
<h1>Least Squares</h1>
The least squares method in matrix form:
$$ b = (T\space X\space X)^{-1} T\space X\space y $$

<br><br>
<h1>Fundamental Theorem of Calculus</h1>
FToC can be written as:
$$ \int_{a}^b d(r) \nabla \space \lambda(r) = \lambda(b) - \lambda(a) $$

