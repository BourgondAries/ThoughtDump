Math. Notation.
<br><br>
<h1>New Home</h1>
Mathematics is plagued by inconsistencies that obscure mathematical meaning. One such example is the way we write matrix multiplication. The standard notation is write a juxtaposition of matrices: $AB$, but this makes it hard to distinguish multi-letter symbols. Thus, some authors choose $A\cdot B$, but this is inconsistent with the dot-product for vectors, in which the first argument is transposed: $A\cdot B = T(A) \space B$. Another inconsistency is the use of the exponentiation operator. Where a transpose is often written $A^T$. The exponential is supposed to mean repetition of the same expression. The laplacian operator breaks this: $\nabla^2 = \nabla\cdot\nabla$ while also using the vector dot-product inconsistency. Inconsistencies within this realm are common.
In addition to solving inconsistencies, we must allow the use of multi-letter symbols to make mathematics easier to read.
<br>
I propose a simple operator based and tensor friendly mathematics that solves these problems.

<br><br>
<h1>Symbols</h1>
A symbol is a string of characters in any language. They are always grouped close together. They can consist of any characters except digits and delimiters.
$$ xyz $$
is a single symbol. $x$ can also be a symbol on its own, but has to stand out. There must be a clear separation like in the expression $x\space y$ or $x\cdot y$.

<br><br>
<h1>Operators</h1>
Operators are symbols that are defined as operators. An operator operates on its left or right side, or both.
<br>
Postfix operator: $ ! $ as in $ 5! $ or $ (n)! $.<br>
Prefix operator: $ \partial_1 $ as in $ \partial_1 f $.<br>
Binary operators: $ + $ as in $ 1 + 2 $.<br>

<br><br>
<h1>Tensors and Tuples</h1>
Tensors are written using square brackets.<br>
Rank 1 tensor:
$$ [x_1, x_2, x_3] $$
Rank 2 tensor:
$$ \begin{bmatrix}
x_{11}       & x_{12} \\
x_{21}       & x_{22}
\end{bmatrix}
$$
Rank 3 tensor:
$$
\begin{bmatrix}
[x_{111}, x_{112}] & [x_{121}, x_{122}] \\
[x_{211}, x_{212}] & [x_{221}, x_{222}]
\end{bmatrix}
$$
And so on.<br>
Tuples denote column vectors: $ (x_1, x_2, ..., x_n) = T[x_1, x_2, ..., x_n] $. Tuples are written using parentheses. A single-element tuple is written $ (x,) $. The T operator denotes transposition. Vectors and tuples may have trailing commas.

<br><br>
<h1>Tensor Extractor</h1>
The tensor extractor is an operator that extracts an entry from a tensor.
$$ \lambda $$
is the tensor extraction operator.
$$ \lambda_3 $$
Denotes the third element of the tensor:
$$ \lambda_3 (x,y,z) = z $$
where $ \lambda(x,y,z) $ is a tuple. A tuple must by necessity be a column vector.
Commas in the tensor extractor denote dimension:
$$ \lambda_{2,3}
\begin{bmatrix}
a, & b, & c \\
d, & e, & f \\
g, & h, & i
\end{bmatrix}
=
f
$$
Tensor extractors can also operate on ranges, given by colons:
$$ \lambda_{2,2:3}
\begin{bmatrix}
a, & b, & c \\
d, & e, & f \\
g, & h, & i
\end{bmatrix}
=
[e, f]
$$
Omitted trailing dimensions means "all entries":
$$ \lambda_{2}
\begin{bmatrix}
a, & b, & c \\
d, & e, & f \\
g, & h, & i
\end{bmatrix}
=
[d, e, f]
$$
To access all elements in previous dimensions:
$$ \lambda_{:,2}
\begin{bmatrix}
a, & b, & c \\
d, & e, & f \\
g, & h, & i
\end{bmatrix}
=
\begin{bmatrix}
b \\
e \\
h
\end{bmatrix}
$$
Alternatively one can use implicit end values via omittance:
$$ \lambda_{2:,2}
\begin{bmatrix}
a, & b, & c \\
d, & e, & f \\
g, & h, & i
\end{bmatrix}
=
\begin{bmatrix}
e \\
h
\end{bmatrix}
$$
This is particularly useful when the size is not known beforehand. All indices start at 1. Negative indices can be used, where $-1$ is the last element in that dimension.
You can also specify multiple ranges using a semicolon:
$$ \lambda_{2:,1;3:}
\begin{bmatrix}
a, & b, & c \\
d, & e, & f \\
g, & h, & i
\end{bmatrix}
=
\begin{bmatrix}
d, & f \\
g, & i
\end{bmatrix}
$$
Extracting the entire tuple is a matter of specifying an all-dimensional element:
$$ \lambda_{:}
\begin{bmatrix}
a, & b, & c \\
d, & e, & f \\
g, & h, & i
\end{bmatrix}
=
\begin{bmatrix}
a, & b, & c \\
d, & e, & f \\
g, & h, & i
\end{bmatrix}
$$

<br><br>
<h1>Functions</h1>
Functions are expressions that map tensors to tensors using tensor extractors. A function always refers to its parameters via those extractors. A function is always a right-facing operator.
<br>
Example 1:
$$ F = \lambda_1 $$
This function maps the first argument of its tuple. If we supply a tuple, we get $ F(x) = x $.

<br>
Example 2:
$$ F = \lambda_2 $$
This function maps the second argument of its tuple. If we supply a tuple, we get $ F(x, y) = y $.

<br>
Example 3:
$$ F = \lambda_2 - \lambda_1 $$
If we supply a tuple, we get $ F(x, y) = y - x $.

<br>
Example 4:
$$ F = \lambda(\lambda_2 \lambda_1) $$
If we supply a tuple, we get $ F(x, y) = y\cdot x $. Notice how we provide an outer $ \lambda $. The reason for doing this is to apply insertion of A into all extractors:
$$ \lambda(\lambda_2\lambda_1)A \neq \lambda_2\lambda_1A $$
as
$$ \lambda(\lambda_2\lambda_1)A = A_2A_1 \neq \lambda_2\lambda_1A = \lambda_2A_1 = A_{1,2} $$

<br>
Example 5:
$$ F = \lambda(\bar{\lambda}_2 A\space \lambda_1) $$
If we supply a tuple, we get $ F(x) = A_2 x $. Notice how we provide a $ \bar{\lambda} $. This symbol denotes eager evaluation and won't be filled in by the outer $\$$.

<br><br>
<h1>Implicit Pattern</h1>
The pattern
$$ \lambda(X\space \lambda_:) $$
can always be written as
$$ X $$
since there is no semantic difference.

<br><br>
<h1>Sums and Products</h1>
Sums and products are operators:

$$ \sum_{i=n}^N = \lambda(\lambda_:(i) + \lambda_:(i+1) + ... + \lambda_:(N)) $$

$$ \prod_{i=n}^N = \lambda(\lambda_:(i) \cdot \lambda_:(i+1) \cdot ... \cdot \lambda_:(N)) $$

<br><br>
<h1>Partial Derivatives</h1>
Partial derivatives are operators that operate on a function with respect to a parameter index:

$$ \partial_1 \lambda(\lambda_1 \lambda_2) = \lambda(\lambda_2) $$

<br><br>
<h1>Total Derivatives</h1>
The total derivative operator is $d$. It is defined as:
$$ d = $\left(\sum_{i=0}^\infty \partial_i \lambda_: d(i)\right) $$

<br><br>
<h1>Nabla</h1>
The nabla operator $\nabla$ is defined as:
$$ \nabla = (\partial_1, \partial_2, ...) $$
It is a column vector of all partial derivative operators.<br>
The operator can be limited by an extractor:
$$ \bar{\lambda}_{:3}\nabla = (\partial_1, \partial_2, \partial_3) $$
For convenience we define $\bar{\nabla}$ to avoid cumbersome expressions:
$$ \bar{\nabla} = \lambda(\bar{\lambda}_{:size(\lambda_:)}\nabla \space \lambda_:) $$

<br><br>
<h1>Gradient</h1>
The gradient is defined by:
$$ grad = \bar{\nabla} $$

<br><br>
<h1>Divergence</h1>
The divergence is defined by:
$$ div = T \space \nabla $$

<br><br>
<h1>Curl</h1>
The curl is defined by:
$$ curl = \bar{\lambda}_{1:3}\nabla \times $$
where $\times$ is the cross product.

<br><br>
<h1>Cross Product</h1>
The cross product is defined only for a 3-vector:
$$ \lambda(\lambda_1 \times \lambda_2) = \lambda(\det(T(\bar{\lambda}_{1:3}\nabla), T\space \lambda_1, T\space \lambda_2)) $$

<br><br>
<h1>Determinant</h1>
The determinant is defined by the $det$ operator:
$$ \det = \lambda\left(\sum_{i=1}^{cols(\lambda_:)}{(-1)^{i-1}\lambda_{1,i}\det(\lambda_{2:,:i-1;i+1:})}\right) $$

<br><br>
<h1>Exponentiation</h1>
Exponents always mean repetition on symbols, otherwise exponentiation:
$$ \sin^3 = \sin\sin\sin $$

<br><br>
<h1>Jacobian</h1>
The Jacobian matrix is defined in terms of nabla:
$$ J = \bar{\nabla}\space T $$

<br><br>
<h1>Hessian</h1>
The Hessian matrix is defined in terms of nabla:
$$ H = \bar{\nabla}(T \space\bar{\nabla}) $$

<br><br>
<h1>Laplacian</h1>
The laplacian operator is defined as:
$$ L = (T\space \nabla) \nabla $$

<br><br>
<h1>Integrals</h1>
Integrals are written in the same form as in old mathematics:
$$ \int_{start}^{stop} d(v) $$

<br><br>
<h1>Vector-Vector Implicitation</h1>
An implicitation operation is performed when two elements are juxtaposed. This means separation by parenthesis or space: $ (x) y$ or $x \space y$.<br>
Here is the definition of vector-vector juxtaposition:
$$ (x_1, x_2, ..., x_n)(y_1, y_2, ..., y_n) = (x_1, x_2, ..., x_n) A = (x_1A, x_2A,...x_nA) $$
The reason for this is to have consistency between vector-valued functions and their tuple-arguments:
$$ f = (f_1, f_2,..., f_n) $$
$$ \Rightarrow f(x,y,z) = (f_1, f_2,..., f_n)(x,y,z) = (f_1(x,y,z), f_2(x,y,z),..., f_n(x,y,z)) $$

<br><br>
<h1>Diagonalization</h1>
Diagonalization of a vector:
$$ diag $$
Operates on a column vector (tuple) and produces a symmetric matrix using the values of the vector as diagonals. If a matrix is given, the vector is extracted from the diagonals.

<br><br>
<h1>Applications</h1>
We'll explore a few applications of the above defintions.

<br><br>
<h1>Taylor Polynomial</h1>
The general taylor polynomial can now be written as:
$$ Y = \lambda^2\left(\sum_{n=0}^\infty \frac{(T\space \lambda^2_1\space \nabla\space)^n\lambda_1}{n!}(\lambda_2)\right) $$
This can be used as:
$$ Y(f, a)(x) \approx f(x) $$
or
$$ f(x-a) \sim \sum_{n=0}^\infty \frac{(T\space (x-a)\space \nabla\space)^nf}{n!}(a) $$
$Y$ creates a new operator. The exponent of $\$$ denotes escaped insertion.

<br><br>
<h1>Fourier/Laplace Transform</h1>
The transforms can be written:
$$ F = \int_{-\infty}^{\infty} d(t) e^{-i\space w\space t} $$
$$ L = \int_{0}^{\infty} d(t) e^{-s\space t} $$

<br><br>
<h1>Least Squares</h1>
The least squares method in matrix form:
$$ b = (T\space X\space X)^{-1} T\space X\space y $$

<br><br>
<h1>Statistics</h1>
The expected value can be defined as:
$$ E = \lambda \left(\sum_{i=1}^\infty \lambda_{1,i} \lambda_{2,i}\right) $$
$$ Var = \lambda \left(\sum_{i=1}^\infty (\lambda_{i}-E(\lambda_{:}))^2 P(\lambda_{i})\right) $$

<br><br>
<h1>Dual Vector Space</h1>
The dual vector space of a basis $V = (v_1, v_2,...,v_n)$ can be expressed as:
$$ f : V \rightarrow \mathbb{R} $$
where
$$ f_i(v_i) = \delta_{i,j} $$
with
$$ f = (a_1f_1, a_2f_2,...,a_nf_n) $$

<br><br>
<h1>Fundamental Theorem of Calculus</h1>
FToC can be written as:
$$ \int_{a}^b d(r) \nabla\space \lambda_:(r) = \lambda(\lambda_:(b) - \lambda_:(a)) $$

<br><br>
<h1>Stokes' Theorem</h1>
Stokes' theorem can be written as:
$$ \int_M d\space w = \int_{\partial M} w $$
Suppose we have the differential form:
$$ w = \lambda(\lambda_{1,1}d\space \lambda_{2,2}\wedge \lambda_{2,3} + \lambda_{1,2}d\space \lambda_{2,3} \wedge \lambda_{2,1} + \lambda_{1,3}d\space \lambda_{2,1}\wedge \lambda_{2,2}) $$
The differential of $w$:
$$ d\space w = d\space \lambda(\lambda_{1,1}d\space \lambda_{2,2}\wedge \lambda_{2,3} + \lambda_{1,2}d\space \lambda_{2,3} \wedge \lambda_{2,1} + \lambda_{1,3}d\space \lambda_{2,1}\wedge \lambda_{2,2}) $$
And thus:
$$ d\space w(F,R) = d\space (F_1d(R_2\wedge R_3) + F_2d(R_3 \wedge R_1) + F_3d(R_1\wedge R_2)) $$
Applying the derivative operator to the functions:
$$ d\space R_1 = \partial_1 R_1d(1) + \partial_2 R_1 d(2) $$
$$ \Rightarrow d\space R_1\wedge\space d\space R_2 = (\partial_1 R_1d(1) + \partial_2 R_1 d(2))\wedge(\partial_1 R_2d(1) + \partial_2 R_2 d(2)) $$
$$ \Rightarrow d\space R_1\wedge\space d\space R_2 = \partial_1R_1d(1)\wedge \partial_2R_2d(2) + \partial_2R_1d(2)\wedge \partial_1R_2d(1) $$
$$ \Rightarrow d\space R_1\wedge\space d\space R_2 = \partial_1R_1\partial_2R_2d(1)\wedge d(2) + \partial_2R_1\partial_1R_2d(2)\wedge d(1) $$
$$ \Rightarrow d\space R_1\wedge\space d\space R_2 = \partial_1R_1\partial_2R_2d(1)\wedge d(2) - \partial_2R_1\partial_1R_2d(1)\wedge d(2) $$
$$ \Rightarrow d\space R_1\wedge\space d\space R_2 = (\partial_1R_1\partial_2R_2 - \partial_2R_1\partial_1R_2) d(1)\wedge d(2) $$
The same can be done for the other wedges:
$$ d\space R_2\wedge\space d\space R_3 = (\partial_1 R_2d(1) + \partial_2 R_2 d(2))\wedge(\partial_1 R_3d(1) + \partial_2 R_3 d(2)) $$
$$ \Rightarrow d\space R_2\wedge\space d\space R_3 = \partial_1R_2d(1)\wedge\partial_2R_3d(2) + \partial_2R_2d(2)\wedge\partial_1R_3d(1) $$
$$ \Rightarrow d\space R_2\wedge\space d\space R_3 = (\partial_1R_2\partial_2R_3 - \partial_2R_2\partial_1R_3)d(1)\wedge d(2) $$
And similarly:
$$ d\space R_3\wedge\space d\space R_1 = (\partial_1 R_3d(1) + \partial_2 R_3 d(2))\wedge(\partial_1 R_1d(1) + \partial_2 R_1 d(2)) $$
$$ \Rightarrow d\space R_3\wedge\space d\space R_1 = (\partial_1R_3\partial_2R_1 - \partial_2R_3\partial_1R_1)d(1)\wedge d(2) $$
Inserting these definitions into Stokes' theorem we get:
$$ \int_V d\left((F_1(\partial_1R_2\partial_2R_3 - \partial_2R_2\partial_1R_3) + F_2(\partial_1R_3\partial_2R_1 - \partial_2R_3\partial_1R_1) +\\ F_3(\partial_1R_1\partial_2R_2-\partial_2R_1\partial_1R_2))d(1)\wedge d(2)\right) = \\ \int_{R} F_1(\partial_1R_2\partial_2R_3 - \partial_2R_2\partial_1R_3) + F_2(\partial_1R_3\partial_2R_1 - \partial_2R_3\partial_1R_1) +\\ F_3(\partial_1R_1\partial_2R_2-\partial_2R_1\partial_1R_2)d(1)\wedge d(2) $$
Where $ R = \partial V$.

<br><br>
<h1>Differential Forms</h1>
A differential 1-form can be defined as:
$$ \omega = \lambda \left(\sum_{i=1}^n\lambda_{1,n}d\space \lambda_{2,n}\right) $$
$$ \Rightarrow \omega(F, R) = \sum_{i=1}^{arity(F)}F_i\space d\space R_i $$
Suppose we want the integral over the 1-manifold $R$ of the vector field $F$, both being in $\mathbb{R}^3$:
$$ \int_R\omega(F,R) = \int_RF_1\space \partial_1R_1\space d(1) + F_2\space \partial_1R_2\space d(1) + F_3\space \partial_1R_3\space d(1) $$
$$ \int_R\omega(F,R) = \int_R(F_1\space \partial_1R_1 + F_2\space \partial_1R_2 + F_3\space \partial_1R_3)d(1) $$
The differential 2-form over a 2-manifold is similar:
