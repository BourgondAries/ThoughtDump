Math. Notation.
<br><br>
<h1>New Home</h1>
Mathematics is plagued by inconsistencies that obscure mathematical meaning. One such example is the way we write matrix multiplication. The standard notation is write a juxtaposition of matrices: $AB$, but this makes it hard to distinguish multi-letter symbols. Thus, some authors choose $A\cdot B$, but this is inconsistent with the dot-product for vectors, in which the first argument is transposed: $A\cdot B = T(A) \space B$. Another inconsistency is the use of the exponentiation operator. Where a transpose is often written $A^T$. The exponential is supposed to mean repetition of the same expression. The laplacian operator breaks this: $\nabla^2 = \nabla\cdot\nabla$ while also using the vector dot-product inconsistency. Another inconsistency is when one integrates over the differential k-form where argument insertion is implied. This implicitness in some of mathematics makes it difficult. Inconsistencies within this realm are common.
In addition to solving inconsistencies, we must allow the use of multi-letter symbols to make mathematics easier to read.
<br>
I propose a simple lambda-calculus based mathematics that solves these problems.

<br><br>
<h1>Symbols</h1>
A symbol is a string of characters in any language. They are always grouped close together. They can consist of any characters except digits and delimiters.
$$ xyz $$
is a single symbol. $x$ can also be a symbol on its own, but has to stand out. There must be a clear separation like in the expression $x\space y$ or $x\cdot y$.

<br><br>
<h1>Operators</h1>
Operators are symbols that are defined as operators. An operator operates on its left or right side, or both.
<br>
Postfix operator: $ ! $ as in $ 5! $ or $ (n)! $.<br>
Prefix operator: $ \partial_1 $ as in $ \partial_1 f $.<br>
Binary operators: $ + $ as in $ 1 + 2 $.<br>

<br><br>
<h1>Tensors and Tuples</h1>
Tensors are written using square brackets.<br>
Rank 1 tensor:
$$ [x_1, x_2, x_3] $$
Rank 2 tensor:
$$ \begin{bmatrix}
x_{11}       & x_{12} \\
x_{21}       & x_{22}
\end{bmatrix}
$$
Rank 3 tensor:
$$
\begin{bmatrix}
[x_{111}, x_{112}] & [x_{121}, x_{122}] \\
[x_{211}, x_{212}] & [x_{221}, x_{222}]
\end{bmatrix}
$$
And so on.<br>
Tuples are written using parentheses: $ (x_1, x_2, ..., x_n) $. A single-element tuple is written $ (x) $ or just $ x $.

<br><br>
<h1>Transpose</h1>
The $ T $ operator denotes the transpose of a tensor.

<br><br>
<h1>Functions</h1>
Functions are expressions that map mathematical objects. A function always refers to its parameters via lambda enumerators. A function is always a right-facing operator.
<br>
Example 1:
$$ F = \lambda_1 = \lambda $$
This function maps the first argument to itself. If we supply a tuple, we get $ F(x) = x $.
No subscript on the lambda means $ \lambda_1 $.

<br>
Example 2:
$$ F = \lambda_2 $$
This function maps the second argument of its tuple. If we supply two arguments we get $ F(x)(y) = F(x, y) = y $.

<br>
Example 3:
$$ F = \lambda_2 - \lambda_1 $$
If we supply one argument, we get $ F(x) = \lambda_1 - x $. Every time an argument is provided, $ \lambda_1 $ is replaced by the argument. All other $ \lambda_n $ are decremented.
If we supply two arguments, we get $ F(x)(y) = F(x, y) = y - x $.

<br>
Example 4:
$$ F = \lambda_2 \lambda_1 $$
If we supply a tuple, we get $ F(x)(y) F(x, y) = y\cdot x $.

<br>
Example 5:
$$ F = A_2 \space \lambda_1 $$
If we supply an argument, we get $ F(x) = A_2 \space x $.

<br><br>
<h1>Lambda Signifier</h1>
The lambda signifier is given by $ (: $. It is used in expressions where one must denote that this is a function. For single variables we can drop it because it is implicit:
$$ f = \lambda_1 + \lambda_2 = (: \lambda_1 + \lambda_2) $$
Because when using $ f $ in an expression, $ (: $ is implied:
$$ f (9, 3) = (: f)(9,3) $$
$ (: $ becomes $ ( $ when the expression becomes $ \eta $-reducable.

<br><br>
<h1>$ \eta $-reduction</h1>
The pattern
$$ (: X \space \lambda) $$
can always be written as
$$ X $$
since there is no semantic difference.<br>
This is called an $ \eta $-reduction. We may write:
$$ (: X \space \lambda) \rightarrow_\beta X $$
If and only if $ X $ is a function or can be regarded as such.

<br><br>
<h1>Scope</h1>
A lambda variable is bound to the closest declaration of $ (: $.
$$ (: \lambda \cdot (: 5 + \lambda) \lambda) $$
The first lambda is bound to the outer function. The second lambda bound to the inner function, and the last lambda is bound to the outer function. In this case, the third lambda is passed to the inner function.
$$ (: \lambda \cdot (: 5 + \lambda) \lambda) 3 \rightarrow_\beta (: 3 \cdot (: 5 + \lambda) 3) \rightarrow_\eta (3 \cdot (: 5 + \lambda) 3) \rightarrow_{\beta\eta} (3 \cdot (5 + 3)) $$
We can state the number of lambda variables as well, this is useful if we want to consume a variable and not use it.
$$
(\lambda_{10}: \lambda_9 + \lambda_4)(1,2,3,4,5,6,7,8,9) \\
\rightarrow_\beta (\lambda_9: \lambda_8 + \lambda_3)(2,3,4,5,6,7,8,9) \\
\rightarrow_\beta (\lambda_8: \lambda_7 + \lambda_2)(3,4,5,6,7,8,9) \\
\rightarrow_\beta (\lambda_7: \lambda_6 + \lambda_1)(4,5,6,7,8,9) \\
\rightarrow_\beta (\lambda_6: \lambda_5 + 4)(5,6,7,8,9) \\
\rightarrow_\beta (\lambda_5: \lambda_4 + 4)(6,7,8,9) \\
\rightarrow_\beta (\lambda_4: \lambda_3 + 4)(7,8,9) \\
\rightarrow_\beta (\lambda_3: \lambda_2 + 4)(8,9) \\
\rightarrow_\beta (\lambda_2: \lambda_1 + 4)(9) \\
\rightarrow_\beta (\lambda_1: 9 + 4)(9) \\
= (\lambda: 9 + 1)
$$
A function can also accept arbitrary lambda expressions:
$$ (\lambda : \lambda) (\lambda_2) = (\lambda_2 : \lambda_2) $$

<br><br>
<h1>Tensors II</h1>
Tensors are to be considered functions of tuples. Let's start with vectors:
$$ \left(: \begin{bmatrix} \lambda_1 \\ \lambda_2 \\ \vdots \\ \lambda_n \end{bmatrix}\right) $$
The same can be done for matrices and higher order tensors. The use of this becomes clear when we define the derivative of a function that takes in a vector:
$$ f = A \cdot \lambda $$
where A is an arbitrary matrix, and $ \lambda $ is a column vector. Assume the length of the vector is 3.
$$ \mathrm{d} \space f = \partial_1 f \space \lambda \space \mathrm{d} \space \lambda $$
Let $ V $ be the vector function that transforms a tuple to a vector.
$$ \mathrm{d} \space f \space V = \partial_1 f \space V \space \mathrm{d} \space V $$
$$ \mathrm{d} \space f \space V = (1: A) \space V \space \mathrm{d} \space V $$
$$ \mathrm{d} \space f \space V = A \space \mathrm{d} \space V $$
$$ \mathrm{d} \space f \space V = A \space (\partial_1\space V \space \mathrm{d}\space\lambda_1 + \partial_2\space V \space \mathrm{d}\space\lambda_2 + \partial_3\space V \space \mathrm{d}\space\lambda_3) $$

<br><br>
<h1>Sums and Products</h1>
Sums and products are operators:

$$ \sum_{i=n}^N = (: \lambda(i) + \lambda(i+1) + ... + \lambda(N)) $$

$$ \prod_{i=n}^N = (: \lambda(i) \cdot \lambda(i+1) \cdot ... \cdot \lambda(N)) $$

<br><br>
<h1>Partial Derivatives</h1>
Partial derivatives are operators that operate on a function with respect to a parameter index:

$$ \partial_1 (: \lambda_1 \lambda_2) = (: \lambda_2) $$

$$ \partial_1 (: \exp(\lambda_1 \cdot a)) = (: a\space \exp(\lambda_1 \cdot a)) $$

<br><br>
<h1>Total Derivatives</h1>
The total derivative operator is $d$. It is defined as:
$$ \mathrm{d} = \sum_{i=1}^{arity(\lambda_1)} \partial_i \lambda_1 \lambda_{2:arity(\lambda_1)+1} \mathrm{d}(\lambda_{i+1}) $$
If we derive the expression $ f \space g $, where $ g $ is a function of multiple variables:
$$ \mathrm{d} \space (f \space g) = \sum_{i=1}^{3} \partial_i (f \space g) \lambda_{1:3} \mathrm{d}(\lambda_{i}) $$
$$ \mathrm{d} \space (f \space g) (x, y, z) = \sum_{i=1}^{3} \partial_i (f \space g) (x, y, z)  \mathrm{d}((x, y, z)_i) $$
I need a better formulation. I'm missing the recursive part.
In fact, it would be most appropriate to call:
$$ \mathrm{d} \space f (g(h(j(x,y,z)))) $$
Now I think the problem comes when we try to define stuff partially: We can't do
$$ \mathrm{d} \space f (g) $$
because it means $ g $ is our final variable.
The definition only accepts two arguments. How can it be n-ary?
One method is to do the following:
$$ \mathrm{d} \space f (g \space \lambda_{1:arity(g)}) $$
You can postpone defining lambda until a later point.

<br><br>
<h1>Apply</h1>
We can define the apply-function as:
$$ A = (\lambda_1: \lambda_1 \space \lambda_{2:arity(\lambda_1) + 1} ) $$

<br><br>
<h1>Nabla</h1>
The nabla operator $\nabla$ is defined as:
$$ \nabla = (\partial_1, \partial_2, ...) $$
It is a column vector of all partial derivative operators.<br>
The operator can be limited by an extractor:
$$ \bar{\lambda}_{:3}\nabla = (\partial_1, \partial_2, \partial_3) $$
For convenience we define $\bar{\nabla}$ to avoid cumbersome expressions:
$$ \bar{\nabla} = \bar{\lambda}_{:arity(\lambda)}\nabla \space \lambda $$

<br><br>
<h1>Gradient</h1>
The gradient is defined by:
$$ grad = \bar{\nabla} $$

<br><br>
<h1>Divergence</h1>
The divergence is defined by:
$$ div = T \space \nabla $$

<br><br>
<h1>Curl</h1>
The curl is defined by:
$$ curl = \bar{\lambda}_{1:3}\nabla \times $$
where $\times$ is the cross product.

<br><br>
<h1>Cross Product</h1>
The cross product is defined only for a 3-vector:
$$ \lambda(\lambda_1 \times \lambda_2) = \lambda(\det(T(\bar{\lambda}_{1:3}\nabla), T\space \lambda_1, T\space \lambda_2)) $$

<br><br>
<h1>Determinant</h1>
The determinant is defined by the $det$ operator:
$$ \det = \lambda\left(\sum_{i=1}^{cols(\lambda_:)}{(-1)^{i-1}\lambda_{1,i}\det(\lambda_{2:,:i-1;i+1:})}\right) $$

<br><br>
<h1>Exponentiation</h1>
Exponents always mean repetition on symbols, otherwise exponentiation:
$$ \sin^3 = \sin\sin\sin $$

<br><br>
<h1>Jacobian</h1>
The Jacobian matrix is defined in terms of nabla:
$$ J = \bar{\nabla}\space T $$

<br><br>
<h1>Hessian</h1>
The Hessian matrix is defined in terms of nabla:
$$ H = \bar{\nabla}(T \space\bar{\nabla}) $$

<br><br>
<h1>Laplacian</h1>
The laplacian operator is defined as:
$$ L = (T\space \nabla) \nabla $$

<br><br>
<h1>Integrals</h1>
Integrals are written in the same form as in old mathematics:
$$ \int_{start}^{stop} d(v) $$

<br><br>
<h1>Vector-Vector Implicitation</h1>
An implicitation operation is performed when two elements are juxtaposed. This means separation by parenthesis or space: $ (x) y$ or $x \space y$.<br>
Here is the definition of vector-vector juxtaposition:
$$ (x_1, x_2, ..., x_n)(y_1, y_2, ..., y_n) = (x_1, x_2, ..., x_n) A = (x_1A, x_2A,...x_nA) $$
The reason for this is to have consistency between vector-valued functions and their tuple-arguments:
$$ f = (f_1, f_2,..., f_n) $$
$$ \Rightarrow f(x,y,z) = (f_1, f_2,..., f_n)(x,y,z) = (f_1(x,y,z), f_2(x,y,z),..., f_n(x,y,z)) $$

<br><br>
<h1>Diagonalization</h1>
Diagonalization of a vector:
$$ diag $$
Operates on a column vector (tuple) and produces a symmetric matrix using the values of the vector as diagonals. If a matrix is given, the vector is extracted from the diagonals.

<br><br>
<h1>Applications</h1>
We'll explore a few applications of the above defintions.

<br><br>
<h1>Taylor Polynomial</h1>
The general taylor polynomial can now be written as:
$$ Y = \lambda^2\left(\sum_{n=0}^\infty \frac{(T\space \lambda^2_1\space \nabla\space)^n\lambda_1}{n!}(\lambda_2)\right) $$
This can be used as:
$$ Y(f, a)(x) \approx f(x) $$
or
$$ f(x-a) \sim \sum_{n=0}^\infty \frac{(T\space (x-a)\space \nabla\space)^nf}{n!}(a) $$
$Y$ creates a new operator. The exponent of $\$$ denotes escaped insertion.

<br><br>
<h1>Fourier/Laplace Transform</h1>
The transforms can be written:
$$ F = \int_{-\infty}^{\infty} d(t) e^{-i\space w\space t} $$
$$ L = \int_{0}^{\infty} d(t) e^{-s\space t} $$

<br><br>
<h1>Least Squares</h1>
The least squares method in matrix form:
$$ b = (T\space X\space X)^{-1} T\space X\space y $$

<br><br>
<h1>Statistics</h1>
The expected value can be defined as:
$$ E = \lambda \left(\sum_{i=1}^\infty \lambda_{1,i} \lambda_{2,i}\right) $$
$$ Var = \lambda \left(\sum_{i=1}^\infty (\lambda_{i}-E(\lambda_{:}))^2 P(\lambda_{i})\right) $$

<br><br>
<h1>Dual Vector Space</h1>
The dual vector space of a basis $V = (v_1, v_2,...,v_n)$ can be expressed as:
$$ f : V \rightarrow \mathbb{R} $$
where
$$ f_i(v_i) = \delta_{i,j} $$
with
$$ f = (a_1f_1, a_2f_2,...,a_nf_n) $$

<br><br>
<h1>Fundamental Theorem of Calculus</h1>
FToC can be written as:
$$ \lambda(\int_{a}^b d(r) \nabla\space \lambda_:(r)) = \lambda(\lambda_:(b) - \lambda_:(a)) $$

<br><br>
<h1>Stokes' Theorem</h1>
Stokes' theorem can be written as:
$$ \int_M d\space w = \int_{\partial M} w $$
Suppose we have the differential form:
$$ w = \lambda^2(\lambda_{1,1}d\space \lambda_{2,2}\wedge \lambda_{2,3} + \lambda_{1,2}d\space \lambda_{2,3} \wedge \lambda_{2,1} + \lambda_{1,3}d\space \lambda_{2,1}\wedge \lambda_{2,2}) $$
The differential of $w$:
$$ d\space w = d\space \lambda^2(\lambda_{1,1}d\space \lambda_{2,2}\wedge \lambda_{2,3} + \lambda_{1,2}d\space \lambda_{2,3} \wedge \lambda_{2,1} + \lambda_{1,3}d\space \lambda_{2,1}\wedge \lambda_{2,2}) $$
And thus:
$$ d\space w(F,R) = (d\space \lambda(F_1d(R_2\wedge R_3) + F_2d(R_3 \wedge R_1) + F_3d(R_1\wedge R_2))) $$
Applying the derivative operator to the functions:
$$ d\space R_1 = \partial_1 R_1d(1) + \partial_2 R_1 d(2) $$
$$ \Rightarrow d\space R_1\wedge\space d\space R_2 = (\partial_1 R_1d(1) + \partial_2 R_1 d(2))\wedge(\partial_1 R_2d(1) + \partial_2 R_2 d(2)) $$
$$ \Rightarrow d\space R_1\wedge\space d\space R_2 = \partial_1R_1d(1)\wedge \partial_2R_2d(2) + \partial_2R_1d(2)\wedge \partial_1R_2d(1) $$
$$ \Rightarrow d\space R_1\wedge\space d\space R_2 = \partial_1R_1\partial_2R_2d(1)\wedge d(2) + \partial_2R_1\partial_1R_2d(2)\wedge d(1) $$
$$ \Rightarrow d\space R_1\wedge\space d\space R_2 = \partial_1R_1\partial_2R_2d(1)\wedge d(2) - \partial_2R_1\partial_1R_2d(1)\wedge d(2) $$
$$ \Rightarrow d\space R_1\wedge\space d\space R_2 = (\partial_1R_1\partial_2R_2 - \partial_2R_1\partial_1R_2) d(1)\wedge d(2) $$
The same can be done for the other wedges:
$$ d\space R_2\wedge\space d\space R_3 = (\partial_1 R_2d(1) + \partial_2 R_2 d(2))\wedge(\partial_1 R_3d(1) + \partial_2 R_3 d(2)) $$
$$ \Rightarrow d\space R_2\wedge\space d\space R_3 = \partial_1R_2d(1)\wedge\partial_2R_3d(2) + \partial_2R_2d(2)\wedge\partial_1R_3d(1) $$
$$ \Rightarrow d\space R_2\wedge\space d\space R_3 = (\partial_1R_2\partial_2R_3 - \partial_2R_2\partial_1R_3)d(1)\wedge d(2) $$
And similarly:
$$ d\space R_3\wedge\space d\space R_1 = (\partial_1 R_3d(1) + \partial_2 R_3 d(2))\wedge(\partial_1 R_1d(1) + \partial_2 R_1 d(2)) $$
$$ \Rightarrow d\space R_3\wedge\space d\space R_1 = (\partial_1R_3\partial_2R_1 - \partial_2R_3\partial_1R_1)d(1)\wedge d(2) $$
Inserting these definitions into Stokes' theorem we get:
$$ \int_V d\left((F_1(\partial_1R_2\partial_2R_3 - \partial_2R_2\partial_1R_3) + F_2(\partial_1R_3\partial_2R_1 - \partial_2R_3\partial_1R_1) +\\ F_3(\partial_1R_1\partial_2R_2-\partial_2R_1\partial_1R_2))d(1)\wedge d(2)\right) = \\ \int_{R} F_1(\partial_1R_2\partial_2R_3 - \partial_2R_2\partial_1R_3) + F_2(\partial_1R_3\partial_2R_1 - \partial_2R_3\partial_1R_1) +\\ F_3(\partial_1R_1\partial_2R_2-\partial_2R_1\partial_1R_2)d(1)\wedge d(2) $$
Where $ R = \partial V$.
We now need to expand the lhs by deriving it.

<br><br>
<h1>Kleene-Rosser Paradox</h1>
The lambda calculus is fundamentally inconsistent as shown by the Kleene-Rosser paradox:
$$ k = \lambda(\neg(\space \lambda_: \space \lambda_:)) $$
$$ \Rightarrow k \space k = \neg(k \space k) $$
We can interpret this as recursion:
$$ kk = p = \neg p = \neg^2 p = ... $$

<br><br>
<h1>Turing Completeness</h1>
Lambda calculus is Turing complete:
$$ \Omega \equiv \lambda^2(\lambda \lambda_: \space \lambda_:^2 \space \lambda \lambda_:) $$
$$ T \equiv \Omega \space \Omega $$
Where $t$ is a single-step Turing interpreter:
$$ T \space t \space (input) \rightarrow_\beta t \space t \space ... \space t \space (input) $$
simulates the turing machine.<br>

<br><br>
<h1>Semantics</h1>
The semantics of lambda expressions are as follows:<br>
1. If a beta reduction contains a lambda accessor, the entire expression shall be a lambda expression.
$$ \lambda(\nabla \space \lambda_:) A \rightarrow_\beta \nabla \space A $$
$$ (\lambda(\nabla \space \lambda_:) \lambda(\lambda_:)) A \rightarrow_\beta \lambda(\nabla \space \lambda(\lambda_:)) A $$
Where $ \lambda(\nabla \space \lambda_:) = \lambda((\partial_1, \partial_2, ...) \lambda_:)) $
$$ (\lambda \space \lambda_:) (\lambda \space \sin \space \lambda_:) (A) $$
$$ (\lambda \space \sin \space \lambda_:) (A) $$
$$ (\lambda \space \lambda_:) (\sin \space A) $$
Left-recursive:
$$ (\lambda_: + 1) (\sin \space \lambda_1 + \cos \space \lambda_2) (a, b) $$
$$ \Rightarrow ((\sin \space \lambda_1 + \cos \space \lambda_2) + 1) (a, b) $$
$$ \Rightarrow ((\sin \space a + \cos \space b) + 1) $$
Right-recursive:
$$ (\lambda_: + 1) (\sin \space \lambda_1 + \cos \space \lambda_2) (a, b) $$
$$ (\lambda_: + 1) (\sin \space a + \cos \space b) $$
$$ ((\sin \space a + \cos \space b) + 1)  $$
The question is whether this is a desired syntax:
$$ d = \sum_{i=1}^\infty \partial_i \lambda_1 \space d \space \lambda_2 $$
This doesn't allow one to index a tuple like earlier, but should be fine using:
$$ second = (\lambda)_{i,j} $$
Or a similar accessor function.
Using the above definition:
$$ \mathrm{d} \space \sin \space x = (\partial_1 \sin \space \mathrm{d} \space \lambda) x = \partial_1 \sin \space \mathrm{d} \space x $$
My question is whether we can now properly nest expressions:
$$ (a + (b \cdot \lambda) \lambda) A = (a + (b \cdot A) A) $$
$$ (a + (b \cdot \lambda_2) \lambda) A = (a + (b \cdot \lambda) A) $$
Does the second lambda now become a value of $ A $ or do we need to apply another argument?
$$ (a + (b \cdot \lambda) \lambda) A = (a + (b \cdot A) A) $$
$$ (:a + (b \cdot \lambda_2) \lambda) A = (a + (b \cdot \lambda) A) $$
It is indeed true that the original lambda calculus can refer to variables outside its own
function via lexical scoping. So should my calculus reflect this? I'd prefer to keep variable indices.
Idea:
$$ (x: x_1 + (y: y_1 \cdot x_1)x_2) (1, 2) (3) $$
No that doesn't work. Besides, lexical scoping shouldn't be suggested since it's something that breaks locality:
$$ (: \lambda_1 + (: \lambda_2 \cdot \lambda_1)\lambda_2) (1, 2) (3) $$
Trying to define d again:
$$ \mathrm{d} = \sum_{i=1}^\infty \partial_i \lambda_1 \space \lambda_2 \space \mathrm{d} \space (\lambda_2)_i $$
$$ \sum_{i=n}^N = \lambda_1(n) + \lambda_1(n+1) + ... + \lambda_1(N) $$
$$ \mathrm{d} = (:\lambda_1(n) + \lambda_1(n+1) + ... + \lambda_1(N)) (:\partial_\lambda_1 \lambda_2 \space \lambda_3 \space \mathrm{d} \space (\lambda_3)_\lambda_1) $$
$$ \mathrm{d} = \partial_1 \lambda_1 \space \lambda_2 \space \mathrm{d} \space (\lambda_2)_1 + \partial_2 \lambda_1 \space \lambda_2 \space \mathrm{d} \space (\lambda_2)_2 + ... $$
$$ \mathrm{d} \space \sin = (:\partial_1 \lambda_1 \space \lambda_2 \space \mathrm{d} \space (\lambda_2)_1 + \partial_2 \lambda_1 \space \lambda_2 \space \mathrm{d} \space (\lambda_2)_2 + ...) \sin $$
$$ \mathrm{d} \space \sin = (:\partial_1 \sin \space \lambda_1 \space \mathrm{d} \space (\lambda_1)_1) $$
What about nabla-like syntax for various functions?
$$ \nabla = (\partial_1 \space \lambda, \partial_2 \space \lambda, ...) $$
$$ T \space \nabla = (: T \space \lambda)(: (\partial_1 \space \lambda, \partial_2 \space \lambda, ...)) $$
Sums can thus be made anonymous:
$$ \sum_n^N \lambda = n + (n+1) + ... + N $$
Let's try deriving a function like:
$$ f = \lambda_1 \space \lambda_2 $$
$$ \mathrm{d} \space f \space x \space y = \partial_1 f \space \mathrm{d} \space x + \partial_2 f \space \mathrm{d} \space y $$
$$ \mathrm{d} \space f \space x \space y = (: \lambda_2) x \space y \space \mathrm{d} \space x + (: \lambda_1) x \space y \space \mathrm{d} \space y $$
$$ \mathrm{d} \space f \space x \space y = y \space \mathrm{d} \space x + x \space \mathrm{d} \space y $$
I'm trying to formalize all this but it's difficult.
<br>
What exactly is a derivative? I don't understand it anymore. Let's try defining the derivative for various functions.<br>
One variable:
$$ \mathrm{d} = \partial_1 \space \lambda \space \lambda_2 \space \mathrm{d} \space \lambda_2 $$
Two variables:
$$ \mathrm{d} = \partial_1 \space \lambda \space \lambda_2 \space \lambda_3 \space \mathrm{d} \space \lambda_2 + \partial_2 \space \lambda \space \lambda_2 \space \lambda_3 \space \mathrm{d} \space \lambda_3 $$
This is not compatible with the notation $ f(x,y) $ when $ (x,y) $ is a tuple. In my case, we actually provide two distinct arguments, and not a tuple. This discrepancy can be solved by simply stating that the aforementioned is not a tuple but rather an argument list that implicitly curries.
This would require an explicit decoupling from tensors.
$$ \mathrm{d} = \partial_1 \space \lambda \space \lambda_2 \space \mathrm{d} \space (\lambda_2)_1 + \partial_2 \space \lambda \space \lambda_2 \space \mathrm{d} \space (\lambda_2)_2 $$
How can we unify the notion of a derivative with the jacobian matrix? That one is basically the same as $ \bar{\nabla} \space T \space f $.
Suppose we are given $ f = T \space \lambda_1 \space \lambda_2 $:
$ \mathrm{d} \space f = (: T \space 1_: \space \lambda_2) \lambda_2
Verily, we can't make () a column vector simply because doing $ f (x) $ with $ f $ being a matrix and $ (x) $ being a vector (imagine x as multiple elements) would cause a matrix-product to occur. Not good. So a tuple has to be distinct.
